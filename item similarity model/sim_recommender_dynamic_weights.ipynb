{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UOCMzMibBPjx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import pytz\n",
    "from operator import itemgetter\n",
    "import fasttext\n",
    "import csv\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "base_path = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Jaem0Ak0BPj4",
    "outputId": "1fa951aa-afdd-41e8-d36c-cc7a771e48cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item columns ['itemID', 'title', 'author', 'publisher', 'main_topic', 'subtopics']\n"
     ]
    }
   ],
   "source": [
    "orig_items = pd.read_csv(\n",
    "    f\"{base_path}data/items.csv\", \n",
    "    header=0, encoding='utf-8', sep='|', quoting=csv.QUOTE_NONE\n",
    ")\n",
    "orig_items['author'] = orig_items['author'].fillna('Unknown')\n",
    "\n",
    "test = pd.read_csv(f\"{base_path}data/evaluation.csv\").join(orig_items.set_index('itemID'), on='itemID').rename(columns={\"main topic\": \"main_topic\"})\n",
    "\n",
    "train_data = pd.read_csv(f\"{base_path}data/train_data.csv\")\n",
    "# loading train items without any duplicates\n",
    "items = pd.read_pickle(f\"{base_path}data/items_v2.pkl\").rename(columns={\"main topic\": \"main_topic\"})\n",
    "\n",
    "print(f'Item columns {items.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect language and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OkV7Ia9tBPj5",
    "outputId": "528e746d-4a6e-4d22-a2de-b069fe43d310"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model('../fast_text_models/lid.176.bin')\n",
    "\n",
    "def langdetect_score(title):\n",
    "        title = title.replace('\\n','')\n",
    "        score = model.predict(title)[1][0]\n",
    "        \n",
    "        return score\n",
    "\n",
    "def langdetect_lang(title):\n",
    "        title = title.replace('\\n','')\n",
    "        language = str(model.predict(title)[0])\n",
    "\n",
    "        return language\n",
    "\n",
    "def trim(language):\n",
    "    language = language.replace(language[:11], '')\n",
    "    language = language[:-3]\n",
    "    \n",
    "    return language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "R1h3R63ZBPj6"
   },
   "outputs": [],
   "source": [
    "items['language'] = items['title'].apply(langdetect_lang).apply(trim)\n",
    "items['language_score'] = items['title'].apply(langdetect_score)\n",
    "\n",
    "# items.to_csv(\"./item_with_lang.csv\")\n",
    "\n",
    "test['language'] = test['title'].apply(langdetect_lang).apply(trim)\n",
    "test['language_score'] = test['title'].apply(langdetect_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate item-item similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "j_train = train_data.join(orig_items.set_index('itemID'), on='itemId').join(orig_items.set_index('itemID'), on='rec', rsuffix='_rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18173488 18173488\n"
     ]
    }
   ],
   "source": [
    "train_item_ids = train_data.itemId.unique()\n",
    "df1 = orig_items[orig_items.itemID.isin(train_item_ids)].copy()\n",
    "df2 = orig_items.copy()\n",
    "\n",
    "df1['key'] = 0\n",
    "df2['key'] = 0\n",
    "\n",
    "sim_df = df1.merge(df2, on='key', how='outer', suffixes=['_test', '_orig'])\n",
    "print(len(sim_df), len(orig_items) * len(train_item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['itemID_test', 'title_test', 'author_test', 'publisher_test',\n",
       "       'main topic_test', 'subtopics_test', 'key', 'itemID_orig', 'title_orig',\n",
       "       'author_orig', 'publisher_orig', 'main topic_orig', 'subtopics_orig'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwtSUK0IBPj6"
   },
   "outputs": [],
   "source": [
    "def measure_similarity(book1, book2, scorings):\n",
    "    \n",
    "    sim_title = similar(book1.title,book2.title)\n",
    "    \n",
    "    if(sim_title == 1):\n",
    "        return 1\n",
    "    \n",
    "    sim_author = similar(book1.author, book2.author)\n",
    "    \n",
    "    if(sim_author < 0.8 or book1.author == \"Unknown\" or book2.author == \"Unknown\"):\n",
    "        sim_author = 0\n",
    "    \n",
    "    if(similar(book1.language, book2.language) == 1):\n",
    "        sim_lang = book1.language_score\n",
    "    else:\n",
    "        sim_lang = 0\n",
    "    \n",
    "    \n",
    "    sim_main_topic = similar(book1.main_topic, book2.main_topic)\n",
    "    \n",
    "    score = (\n",
    "        (sim_author * scorings['sim_author']) + \n",
    "            (sim_main_topic * scorings['sim_main_topic']) + \n",
    "            (sim_title * scorings['sim_title']) + \n",
    "            (sim_lang * scorings['sim_lang'])\n",
    "    )\n",
    "#     score = sim_author * 0.2 + sim_main_topic * 0.3 + sim_title * 0.3 + sim_lang * 0.2\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def get_top_n_recommendation(test_case, items, n, scorings):\n",
    "    recommendations = []\n",
    "    for index, item in items.iterrows():\n",
    "        score = measure_similarity(item, test_case, scorings)\n",
    "        if(score < 1.0):\n",
    "            recommendations.append(\n",
    "                {\n",
    "                    'test_id': test_case.itemID,\n",
    "                    'rec_id': index,\n",
    "                    'score': round(score, 4)\n",
    "                }\n",
    "            )\n",
    "            \n",
    "    \n",
    "    top_n = sorted(recommendations, key=itemgetter('score'), reverse=True)[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "def save_recommendations(result_df, prefix=None):\n",
    "    # Saving recommendations\n",
    "    if prefix is None:\n",
    "        berlin_now = datetime.datetime.now(pytz.timezone('Europe/Berlin'))\n",
    "        date_time = berlin_now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        prefix = f'{base_path}result/{date_time}'\n",
    "\n",
    "    result_file_name_1 = f'{prefix}_recommendation_ids.csv'\n",
    "    result_file_name_2 = f'{prefix}_recommendations.csv'\n",
    "    \n",
    "    result_df.to_csv(result_file_name_1, index=False)\n",
    "    \n",
    "    result_df_with_details = result_df.join(\n",
    "        orig_items.set_index('itemID'),\n",
    "        on='test_id',\n",
    "        rsuffix='_test'\n",
    "    ).join(\n",
    "        orig_items.set_index('itemID'),\n",
    "        on='rec_id',\n",
    "        rsuffix='_rec'\n",
    "    ).sort_values(by=['test_id', 'score'], ascending=[True, False])\n",
    "    \n",
    "    result_df_with_details.to_csv(result_file_name_2, index=False)\n",
    "\n",
    "\n",
    "    print(f'Recommendations generated successfully. Files: \\n\\t{result_file_name_1}\\n\\t{result_file_name_2}')\n",
    "    \n",
    "    return result_df_with_details\n",
    "\n",
    "\n",
    "def generate_recommendations(test_cases, n=30, scorings={\n",
    "    'sim_author': 0.2,\n",
    "    'sim_title': 0.3,\n",
    "    'sim_main_topic': 0.3,\n",
    "    'sim_lang': 0.2,\n",
    "}):\n",
    "    result = { \n",
    "        'test_id': [],\n",
    "        'rec_id': [],\n",
    "        'score': []\n",
    "    }\n",
    "    \n",
    "    counter = 0\n",
    "    for index, test_case in test_cases.iterrows():\n",
    "        counter = counter + 1\n",
    "        print(f\"({counter}) Working on the test item: id = {test_case['itemID']}, title = {test_case['title']}\")\n",
    "        recommendations = get_top_n_recommendation(test_case, items, n, scorings)\n",
    "\n",
    "        for recommendation in recommendations:\n",
    "            result['test_id'].append(recommendation['test_id'])\n",
    "            result['rec_id'].append(recommendation['rec_id'])\n",
    "            result['score'].append(recommendation['score'])\n",
    "\n",
    "\n",
    "    result_df = pd.DataFrame(result).sort_values(by=['test_id', 'score'], ascending=[True, False])\n",
    "    \n",
    "    print(f'\\n{n} recommendations generated for each of the {len(test_cases)} items\\n')\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = generate_recommendations(test_cases=test.head(5), n=5)\n",
    "\n",
    "result_df_with_details = save_recommendations(result_df, prefix=f'{base_path}result/test')\n",
    "\n",
    "print(result_df.test_id.nunique(), len(result_df))\n",
    "print(result_df_with_details.test_id.nunique(), len(result_df_with_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twxGH6O2BPj8"
   },
   "outputs": [],
   "source": [
    "# sanity_df = pd.read_csv(f\"{base_path}result/test_recommendations.csv\")\n",
    "# sanity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_scoring(cond, scorings, n=5):\n",
    "    print(scorings, sum(scorings.values()))\n",
    "    test_result_df = generate_recommendations(test_cases=test[cond_test], n=n, scorings=scorings)\n",
    "    test_result_df_with_details = save_recommendations(test_result_df, prefix=f'{base_path}result/dummy')\n",
    "    \n",
    "    return test_result_df_with_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ItemID = 14015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current wieghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current evaluation scoring weights\n",
    "cond_test = test.itemID == 14015\n",
    "scorings={\n",
    "    'sim_author': 0.2,\n",
    "    'sim_title': 0.3,\n",
    "    'sim_main_topic': 0.3,\n",
    "    'sim_lang': 0.2,\n",
    "}\n",
    "evaluate_scoring(cond_test, scorings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_scoring(cond_test, scorings, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative scorings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_test = test.itemID == 14015\n",
    "scorings={\n",
    "    'sim_author': 0.20,\n",
    "    'sim_title': 0.60,\n",
    "    'sim_lang': 0.15,\n",
    "    'sim_main_topic': 0.05,\n",
    "}\n",
    "# We should consider publishers, there are more comics from STONE ARCH BOOKS\n",
    "evaluate_scoring(cond_test, scorings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_scoring(cond_test, scorings, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_items[\n",
    "    orig_items.publisher == 'STONE ARCH BOOKS'\n",
    "].title.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.main_topic.str[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_items['main topic'].str[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_items[orig_items['main topic'].str[0] == 'L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Explore_Language.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
